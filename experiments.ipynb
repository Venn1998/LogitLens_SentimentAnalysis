{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2duH3xGU7Vr"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from torch import Tensor\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import accelerate\n",
        "import random\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0u4WWSvc0aw"
      },
      "source": [
        "# SST-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7MK3Y12c7MS"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"sst2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fPJWq07c9Qi"
      },
      "outputs": [],
      "source": [
        "formatting_prompt = \"\"\"Text: {0}\n",
        "Classify the text into negative or positive sentiment.\n",
        "Sentiment: {1}\n",
        "\"\"\"\n",
        "\n",
        "def format_text(text, label):\n",
        "    formatted_text = formatting_prompt.format(text, label)\n",
        "    return formatted_text\n",
        "\n",
        "def read_sst_split(dataset_split):\n",
        "  texts = []\n",
        "  labels = []\n",
        "  for text, label in zip(dataset_split['sentence'],dataset_split['label']):\n",
        "    label = 'positive' if label == 1 else 'negative'\n",
        "    labels.append(label)\n",
        "    texts.append(format_text(text,label))\n",
        "  return texts,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-9X9xZ_c9Un"
      },
      "outputs": [],
      "source": [
        "sst_texts, sst_labels = read_sst_split(dataset['validation'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3pYayhmU7Vu"
      },
      "source": [
        "# Fine tuning gpt2 on IMDB for CLM objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9i-cAmQOpL8"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF9AtdpxFAeF"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4uaLwukFGSy"
      },
      "outputs": [],
      "source": [
        "def read_imdb_split(dataset_split):\n",
        "  texts = []\n",
        "  labels = []\n",
        "  for text, label in zip(dataset_split['text'],dataset_split['label']):\n",
        "    label = 'positive' if label == 1 else 'negative'\n",
        "    labels.append(label)\n",
        "    texts.append(format_text(text,label))\n",
        "  return texts,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJoGFPKrFZ-e"
      },
      "outputs": [],
      "source": [
        "train_texts, train_labels = read_imdb_split(dataset['train'])\n",
        "test_texts, test_labels = read_imdb_split(dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_ioH2cyVxt1",
        "outputId": "fb6a821f-2d67-4ae3-c54b-8aa1f9eb1de5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y-L3378U7Vw",
        "outputId": "0cc09dcb-31ae-4fdf-9666-1d7aa8d50575"
      },
      "outputs": [],
      "source": [
        "train_texts[0], train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEpTMVR6U7Vx"
      },
      "outputs": [],
      "source": [
        "file_path_train = 'drive/MyDrive/train.txt'\n",
        "text_data_train = open(file_path_train, 'w')\n",
        "for text in train_texts:\n",
        "  text_data_train.write(text)\n",
        "text_data_train.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8tgAZvKU7Vy"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = transformers.TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5hBOVi3OxeF"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EzXJCZHU7V0"
      },
      "outputs": [],
      "source": [
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = transformers.DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n",
        "\n",
        "\n",
        "def train(train_file_path,\n",
        "          model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          ):\n",
        "  tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "  model = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = transformers.TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = transformers.Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzQoga8-U7V1"
      },
      "outputs": [],
      "source": [
        "train_file_path = '/content/drive/MyDrive/train.txt'\n",
        "model_name = 'gpt2'\n",
        "output_dir = '/content/drive/MyDrive/fine_tuned_models'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lEW2nyn1U7V1",
        "outputId": "a821b37f-d3d8-4746-ebe5-f19d39720f82"
      },
      "outputs": [],
      "source": [
        "# train(\n",
        "#     train_file_path=train_file_path,\n",
        "#     model_name=model_name,\n",
        "#     output_dir=output_dir,\n",
        "#     overwrite_output_dir=overwrite_output_dir,\n",
        "#     per_device_train_batch_size=per_device_train_batch_size,\n",
        "#     num_train_epochs=num_train_epochs,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_2O7SWAO2G_"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDXLQSW9uQy0"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    model = transformers.GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(tokenizer_path,truncation=True, max_length=1023)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def generate_text(sequence, max_new_tokens, model, tokenizer):\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt', truncation=True, max_length=1023)\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    return tokenizer.decode(final_outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU6IYHWxG9VJ"
      },
      "outputs": [],
      "source": [
        "model_path = output_dir\n",
        "model = load_model(model_path)\n",
        "tokenizer = load_tokenizer(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNMqbBvcRleD"
      },
      "source": [
        "### zero-shot classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrZrkOJZukYj",
        "outputId": "8c27d75f-d968-4af2-c6dc-2e965bd72631"
      },
      "outputs": [],
      "source": [
        "sequence = \"\"\"Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Classify the text into negative or positive sentiment.\n",
        "Sentiment:\"\"\"\n",
        "max_new_tokens = 2\n",
        "print(generate_text(sequence, max_new_tokens, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtZYDrVKvtbd",
        "outputId": "46e87231-e8c2-44a6-c3b0-bd0762150e9d"
      },
      "outputs": [],
      "source": [
        "sequence = \"\"\"Text: A waste of time.\n",
        "Classify the text into negative or positive sentiment.\n",
        "Sentiment:\"\"\"\n",
        "max_new_tokens = 2\n",
        "print(generate_text(sequence, max_new_tokens, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc1x13Z5v01O",
        "outputId": "44b995a5-b25a-4401-a257-5c9c53ace4fc"
      },
      "outputs": [],
      "source": [
        "sequence = \"\"\"Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Classify the text into negative or positive sentiment.\n",
        "Sentiment:\"\"\"\n",
        "max_new_tokens = 2\n",
        "print(generate_text(sequence, max_new_tokens, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "potADdNM2soE"
      },
      "outputs": [],
      "source": [
        "def classification(data, model, tokenizer):\n",
        "  predictions = []\n",
        "  labels = []\n",
        "  separator = '\\nSentiment:'\n",
        "  for sequence in data:\n",
        "    input, label = sequence.split(separator)\n",
        "    generated = generate_text(input+separator,1,model,tokenizer)\n",
        "    try:\n",
        "      predictions.append(generated.split(separator)[1].strip())\n",
        "    except:\n",
        "      print(generated.split(separator))\n",
        "      predictions.append('')\n",
        "    labels.append(label.strip().casefold())\n",
        "  return predictions, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgbhfeaf3ZnX"
      },
      "outputs": [],
      "source": [
        "#IMDB\n",
        "preds, labels = classification(test_texts[:100], model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDd5wfbnEWIK",
        "outputId": "9133bfa6-173a-416d-89c8-c9c5d757cfa5"
      },
      "outputs": [],
      "source": [
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PckmPY8NemD"
      },
      "outputs": [],
      "source": [
        "#SST2\n",
        "preds, labels = classification(sst_texts, model, tokenizer)\n",
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx8uaip0OCok",
        "outputId": "75b05777-4cef-4c42-ee77-998c346a5c36"
      },
      "outputs": [],
      "source": [
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PBF-kjrQR6z"
      },
      "source": [
        "#### One-shot classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-WcalvdQWHa"
      },
      "outputs": [],
      "source": [
        "def prepend_example(text, example):\n",
        "    formatted_text = example.format(text)\n",
        "    return formatted_text\n",
        "\n",
        "example = \"\"\"Text: A waste of time. The plot is very boring and the actors are very bad.\n",
        "Classify the text into negative or positive sentiment.\n",
        "sentiment: negative\n",
        "\n",
        "{0}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYJg99pbQcTg"
      },
      "outputs": [],
      "source": [
        "preds, labels = classification([prepend_example(text,example) for text in sst_texts], model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaQg_3KQR_4j",
        "outputId": "5824a529-fca2-4836-ba73-eff03f334fb9"
      },
      "outputs": [],
      "source": [
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kwqMZqKmsMF"
      },
      "outputs": [],
      "source": [
        "#incorrect example\n",
        "incorrect_example = \"\"\"Text: A waste of time. The plot is very boring and the actors are very bad.\n",
        "Classify the text into negative or positive sentiment.\n",
        "sentiment: positive\n",
        "\n",
        "{0}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twRIMZUsm4oU"
      },
      "outputs": [],
      "source": [
        "preds, labels = classification([prepend_example(text,incorrect_example) for text in sst_texts], model, tokenizer)\n",
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdh-7G26QQMH"
      },
      "source": [
        "# Before fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYUIKkW_QTAY"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2\"\n",
        "model = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7kXjLtcIhRB",
        "outputId": "bb552d7b-2453-4beb-b3a6-1c1384dcd92c"
      },
      "outputs": [],
      "source": [
        "sequence = \"\"\"Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Classify the text into negative or positive sentiment.\n",
        "Sentiment:\"\"\"\n",
        "max_new_tokens = 1\n",
        "print(generate_text(sequence, max_new_tokens, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTWFJmHIIlUb",
        "outputId": "40bdacf7-11e1-46aa-9031-80d4e638d2d9"
      },
      "outputs": [],
      "source": [
        "sequence = \"\"\"Text: A waste of time.\n",
        "Classify the text into negative or positive sentiment.\n",
        "Sentiment:\"\"\"\n",
        "print(generate_text(sequence, max_new_tokens, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJf8dl-iJRHc",
        "outputId": "aea25027-0a6d-4311-e5d3-28080edc7166"
      },
      "outputs": [],
      "source": [
        "sequence = \"\"\"Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Classify the text into negative or positive sentiment.\n",
        "Sentiment:\"\"\"\n",
        "print(generate_text(sequence, max_new_tokens, model, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S46Zv93hkopF"
      },
      "source": [
        "### IMBD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l92O9XTKQYXO"
      },
      "outputs": [],
      "source": [
        "#IMBD\n",
        "preds, labels = classification(test_texts[:100], model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAbfwSKsQfTT",
        "outputId": "a5de8e89-af3b-434d-cdc0-c577af7593f8"
      },
      "outputs": [],
      "source": [
        "#IMBD\n",
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuuJLVepJYk2"
      },
      "source": [
        "Before fine-tuning, gpt2 is not able to perform sentiment analysis with zero-shot prompting.  \\\\\n",
        "Let's see what happens if we give it one example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVFTr0qxa2j1"
      },
      "outputs": [],
      "source": [
        "def prepend_example(text, example):\n",
        "    formatted_text = example.format(text)\n",
        "    return formatted_text\n",
        "\n",
        "example = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time. The plot is very boring and the actors are very bad.\n",
        "sentiment: negative\n",
        "\n",
        "{0}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uSC7omkdnz4",
        "outputId": "03f1b9bd-b6e3-4625-edd5-d5d88d047bf0"
      },
      "outputs": [],
      "source": [
        "print(generate_text(prepend_example(test_texts[0],example), 1, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL0mQzFWd7ir"
      },
      "outputs": [],
      "source": [
        "preds, labels = classification([prepend_example(text,example) for text in test_texts[:100]], model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydpH3c5seIYf",
        "outputId": "a040e1db-e02f-45c4-8a30-a60155217820"
      },
      "outputs": [],
      "source": [
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjeIHdBJnEdU"
      },
      "outputs": [],
      "source": [
        "#incorrect example\n",
        "incorrect_example = \"\"\"Text: A waste of time. The plot is very boring and the actors are very bad.\n",
        "Classify the text into negative or positive sentiment.\n",
        "sentiment: positive\n",
        "\n",
        "{0}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MmsVnZ4sk4n",
        "outputId": "0eee3890-4325-47e3-9a33-72369c6a501b"
      },
      "outputs": [],
      "source": [
        "#incorrect example\n",
        "preds, labels = classification([prepend_example(text,incorrect_example) for text in test_texts[:100]], model, tokenizer)\n",
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXHm6hjlklPx"
      },
      "source": [
        "### SST-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTXeDwP3kkW1"
      },
      "outputs": [],
      "source": [
        "preds, labels = classification(sst_texts, model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0Ed2r2kllo3",
        "outputId": "2c0ccaa2-8528-4f5c-dc06-32ee2001a2e9"
      },
      "outputs": [],
      "source": [
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh9UIvjulzEq",
        "outputId": "51d65783-2168-45e7-d2fd-c3db709440cb"
      },
      "outputs": [],
      "source": [
        "#one-shot\n",
        "print(generate_text(prepend_example(sst_texts[0]), 1, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzlo2kLXmDRk"
      },
      "outputs": [],
      "source": [
        "preds, labels = classification([prepend_example(text,example) for text in sst_texts], model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivcz1e0LmHOr",
        "outputId": "2696d5df-f582-4a8d-d67e-277b7d7d6e85"
      },
      "outputs": [],
      "source": [
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wp1486CnIgm",
        "outputId": "21d431fe-32aa-452e-fcb6-32db65b04d2e"
      },
      "outputs": [],
      "source": [
        "#incorrect example\n",
        "preds, labels = classification([prepend_example(text,incorrect_example) for text in sst_texts], model, tokenizer)\n",
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1kUZiO_fWft"
      },
      "source": [
        "On IMDB, giving the model one example the accuracy increases a bit, but the model is still far from being able to perform the task. For SST-2 the accuracy increases a lot more, this is expected since the shorter sentences are easier to classify.Interestingly the accuracy increases even more if the moded is provided with an incorrect example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRr431hUQgg8"
      },
      "source": [
        "# Logit Lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SH1lk2lRnCT"
      },
      "outputs": [],
      "source": [
        "#fine-tuned\n",
        "# model_path = output_dir\n",
        "# gpt2 = load_model(model_path)\n",
        "# gpt2_tokenizer = load_tokenizer(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX1ZF43pymy3"
      },
      "outputs": [],
      "source": [
        "#before fine-tuning\n",
        "model_name = \"gpt2\"\n",
        "gpt2 = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
        "gpt2_tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MLT-XzDU7V1",
        "outputId": "d3845d15-7e93-43c8-8444-c4ff11f91da0"
      },
      "outputs": [],
      "source": [
        "print(gpt2.base_model.h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRMlIakcU7V3"
      },
      "source": [
        "For each layer:\n",
        "1. Normalize the output using the final layernorm\n",
        "2. Compute the word distribution using the word embeddings\n",
        "3. Find the most likely token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9bpiRDXU7V4"
      },
      "outputs": [],
      "source": [
        "def compute_logits(\n",
        "        prompt,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        ): # Computes the logits for each token in the vocabulary for each layer of the model.\n",
        "        final_layernorm = model.base_model.ln_f\n",
        "        word_embeddings = model.base_model.wte\n",
        "        tokenized_prompt = tokenizer.encode(prompt, return_tensors = \"pt\") # tokenized prompt\n",
        "\n",
        "\n",
        "        n_layers_model = len(model.base_model.h)  # 12 for GPT2\n",
        "        outputs = [None] * n_layers_model\n",
        "\n",
        "        #store the output of each layer in outputs\n",
        "        def save_output_layer_hook(module, input, output, layer_index):\n",
        "                outputs[layer_index] = output[0].detach()\n",
        "\n",
        "        #add the forward hook to each layer\n",
        "        hooks = [block.register_forward_hook(partial(save_output_layer_hook, layer_index=i))\n",
        "                for i,block in enumerate(model.base_model.h)]\n",
        "\n",
        "        #run the model\n",
        "        try:\n",
        "                with torch.no_grad():\n",
        "                        model(tokenized_prompt)\n",
        "        finally:\n",
        "                for hook in hooks:\n",
        "                        hook.remove()\n",
        "\n",
        "        per_layer_logits = []\n",
        "        per_layer_best_token = []\n",
        "\n",
        "        for layer_output in outputs:\n",
        "\n",
        "                normalized_output = final_layernorm(layer_output) #axs: (batch, tokens, 768)\n",
        "                #word_embeddings.weight.T: (768, 50257) # embedding\n",
        "\n",
        "                # compute the \"cosine similarity\" between the normalized output and the embedding matrix\n",
        "                word_distribution = torch.matmul(normalized_output, word_embeddings.weight.T)[0] #axs: (batch,) token, vocab\n",
        "\n",
        "                best_token = torch.argmax(word_distribution, dim=-1)\n",
        "\n",
        "                per_layer_logits.append(word_distribution)\n",
        "                per_layer_best_token.append(best_token)\n",
        "\n",
        "        per_layer_logits = torch.stack(per_layer_logits)\n",
        "        per_layer_best_token = torch.stack(per_layer_best_token)\n",
        "\n",
        "        return per_layer_logits, per_layer_best_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knXhzKOEU7V4"
      },
      "outputs": [],
      "source": [
        "def get_logits_at_preds(logits, preds): #Float[Tensor, \"layer nb_tokens vocab=50257\"], Int[Tensor, \"nb_tokens\"]\n",
        "    #logits: logits/probabilities of all possible tokens for each layer\n",
        "    #preds: best token for final layer\n",
        "    #logit/probability of the final layer output token for each layer\n",
        "    return np.stack([logits[:, j, preds[j]] for j in range(preds.shape[-1])], axis=-1) #layer, token\n",
        "\n",
        "def plot_logit_lens(\n",
        "    layer_logits, #Float[Tensor, \"layer nb_tokens vocab=50257\"]\n",
        "    layer_preds, #Int[Tensor, \"layer nb_tokens\"]  #best token per layer\n",
        "    layer_probs, #Float[Tensor, \"layer nb_tokens vocab=50257\"] #softmax of logits\n",
        "    tokenizer, #transformers.tokenization_utils.PreTrainedTokenizer\n",
        "    input_ids, #Int[Tensor, \"batch=1 nb_tokens\"]\n",
        "    start_ix=0, #start index of the input_ids #int\n",
        "    layer_names=None,\n",
        "    probability=False,\n",
        "):\n",
        "    input_ids = torch.cat([input_ids, torch.tensor([[50256]])], dim=1)\n",
        "\n",
        "    end_ix = start_ix + layer_logits.shape[1]\n",
        "\n",
        "    final_preds = layer_preds[-1] #Int[Tensor, \"nb_tokens\"] #best token final layer\n",
        "\n",
        "    aligned_preds = layer_preds #Int[Tensor, \"layer nb_tokens\"] #best token per layer\n",
        "\n",
        "    numeric_input = layer_probs if probability else layer_logits #Float[Tensor, \"layer nb_tokens vocab=50257\"]\n",
        "\n",
        "    to_show = get_logits_at_preds(numeric_input, final_preds) #Float[Tensor, \"layer nb_tokens\"]  #logit/prob of the final layer output token for each layer\n",
        "\n",
        "    aligned_texts = []\n",
        "    for layer in per_layer_best_token:\n",
        "        aligned_texts.append([tokenizer.decode(x) for x in layer])\n",
        "\n",
        "    aligned_texts = np.array(aligned_texts) #(layer, token)\n",
        "\n",
        "    to_show = to_show[::-1] #reverse the order of the layers\n",
        "\n",
        "    aligned_texts = aligned_texts[::-1] #reverse the order of the layers\n",
        "\n",
        "    fig = plt.figure(figsize=(1.5 * to_show.shape[1], 0.375 * to_show.shape[0]))\n",
        "\n",
        "    plot_kwargs = {\"annot\": aligned_texts, \"fmt\": \"\"}\n",
        "\n",
        "    if probability:\n",
        "        plot_kwargs.update({\"cmap\": \"Blues_r\",\n",
        "                            \"vmin\": 0,\n",
        "                            \"vmax\": 1})\n",
        "    else:\n",
        "        vmin = np.percentile(to_show.reshape(-1), 5)\n",
        "        vmax = np.percentile(to_show.reshape(-1), 95)\n",
        "\n",
        "        plot_kwargs.update(\n",
        "            {\n",
        "                \"cmap\": \"Blues\",\n",
        "                \"vmin\": vmin,\n",
        "                \"vmax\": vmax,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    sns.heatmap(to_show, **plot_kwargs)\n",
        "\n",
        "    ax = plt.gca()\n",
        "    input_tokens_str = np.array([tokenizer.decode(x) for x in input_ids[0]])\n",
        "\n",
        "\n",
        "    if layer_names is None:\n",
        "        layer_names = [\"Layer {}\".format(n) for n in range(to_show.shape[0])]\n",
        "    ylabels = layer_names[::-1] #reverse the order of the layers\n",
        "    ax.set_yticklabels(ylabels, rotation=0)\n",
        "\n",
        "    ax_top = ax.twiny() #create a twin Axes sharing the yaxis\n",
        "\n",
        "    padw = 0.5 / to_show.shape[1] #padding width\n",
        "    # ax.set_xticks(np.linspace(padw, 1 - padw, to_show.shape[1]))\n",
        "    ax_top.set_xticks(np.linspace(padw, 1 - padw, to_show.shape[1])) #it is needed otherwise the ticks start from 0\n",
        "\n",
        "    ax_inputs = ax\n",
        "    ax_targets = ax_top\n",
        "\n",
        "    ax_inputs.set_xticklabels(input_tokens_str[start_ix:end_ix], rotation=0)\n",
        "\n",
        "    starred = [\n",
        "        \"* \" + true if pred == true else \" \" + true\n",
        "        for pred, true in zip(\n",
        "            aligned_texts[0], input_tokens_str[start_ix + 1 : end_ix + 1]\n",
        "        )\n",
        "    ]\n",
        "    ax_targets.set_xticklabels(starred, rotation=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XqHMmSrnbLi"
      },
      "outputs": [],
      "source": [
        "def logit_lens(prompt,model,tokenizer):\n",
        "  per_layer_logits, per_layer_best_token = compute_logits(prompt, model, tokenizer)\n",
        "  plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    tokenizer,\n",
        "    tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBTGbYIgbg5-"
      },
      "source": [
        "## Before fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duyaEMCWbnKF"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2\"\n",
        "gpt2 = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
        "gpt2_tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62IQ_4CKoA1Z"
      },
      "outputs": [],
      "source": [
        "zero_shot = sst_texts[42]\n",
        "one_shot = prepend_example(prompt,example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "aMLGqddrowqT",
        "outputId": "b76f0644-8d50-4582-9178-863b9108e018"
      },
      "outputs": [],
      "source": [
        "prompt = zero_shot\n",
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "u1UBaaiIpKda",
        "outputId": "9a246404-99b5-4369-8bd0-390bbc989565"
      },
      "outputs": [],
      "source": [
        "prompt = one_shot\n",
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjyLycLMy-Qu"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "1Ga_rVO70Lvu",
        "outputId": "540ab861-1bdc-4da0-df2d-90cb6adc5c80"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxPY3yGxb2Lm"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: positive\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "hfM9G60SR6pn",
        "outputId": "617d561f-87de-43a8-9bed-cb9ef8306010"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_Hj0m9yPz7h"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ESw7L4PGP5mV",
        "outputId": "91580727-ef33-4ad8-dfce-de10f4f20571"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTE1gGLqLFes"
      },
      "source": [
        "Let's see if something changes if we just invert the order of the instruction and the text in the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc-nGdRjKmd9"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Classify the text into negative or positive sentiment.\n",
        "Sentiment: positive\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "ZW6lBfvtKrJf",
        "outputId": "54335551-1b90-4312-9050-12c8db8aedde"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87KWlPQIQKJu"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Classify the text into negative or positive sentiment.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "gAr2mMMJQOvM",
        "outputId": "5c769e7f-5160-4828-844b-555ca99ae91c"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKC9lSWzKQUE"
      },
      "source": [
        "Logit lens lets us see that the model already at some layers produces the correct answer for the sentiment analysis task! Still, it's not able to fully understand the task without examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyuTN_AgLTCo"
      },
      "outputs": [],
      "source": [
        "#one-shot:\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: positive\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "45_MwUnJLhDx",
        "outputId": "54261558-4a13-4987-9226-eaf616a2ffa3"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjcCmxydQbCM"
      },
      "outputs": [],
      "source": [
        "#one-shot:\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "X0rbHG8lQp-M",
        "outputId": "2752872a-8d7a-49a2-f3eb-fefc71994c57"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyIRNuu1PODh"
      },
      "source": [
        "We can see that one example is enough for the model to understand the task and sometimes it gives us the correct answer. The model keeps guessing the correct sentiment in some intermediate layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njZQ9VMSRGMj"
      },
      "outputs": [],
      "source": [
        "# 2 examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "4RGgBG7XRI1X",
        "outputId": "babfd391-2be1-4718-bd83-47ac1226e054"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_89CiUSxUGRh"
      },
      "outputs": [],
      "source": [
        "# 2 examples, inverted order\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "uh-w9VM4ULP0",
        "outputId": "a7a24ad7-1587-4e4b-c26f-084e9fa033b3"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDoxEC9hTkGU"
      },
      "source": [
        "The model has surely understood what it's being asked from the examples, but keeps giving the wrong answer. But logit lens lets us see that the model is more uncertain than when provided with only one example, and shifts between the correct and the incorrect label in the last layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpB0Yenvb-GI"
      },
      "outputs": [],
      "source": [
        "# 3 examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "QGRv6H4HS7fg",
        "outputId": "7a41f8ba-552b-4ff1-8189-c76eeced3f00"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Ja4zHqVg-I"
      },
      "source": [
        "3 examples seem still not enough for the model to output the correct answer. Logit lens lets us see that the model is still uncertain about it, let's see if with a fourth example it becomes better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XGALMqeV7KU"
      },
      "outputs": [],
      "source": [
        "#4 examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "huJOkXWuWC9a",
        "outputId": "c0e75405-ba3d-4baf-8512-14845dbe2af2"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LrvUGJWTGM"
      },
      "source": [
        "Now the model is more sure of the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn9vMLhXVTq_"
      },
      "source": [
        "## Incorrect examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxQtJunoWZdR"
      },
      "outputs": [],
      "source": [
        "#one incorrect example\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "XiCT4pesWj-0",
        "outputId": "3260250e-c71e-45bd-8b5b-e2ce556bd0dd"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s5aHcRRXN-Y"
      },
      "source": [
        "With an incorrect example the model is able to predict the correct label, contrary to what happens with a correct example. This is evidence that the model may be simply guessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq167uRvXvDD"
      },
      "outputs": [],
      "source": [
        "# 2 incorrect examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "XUIseIJWX0I9",
        "outputId": "4ace287c-2710-4d16-f5df-83222c24abe5"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdOgOlBrYt0C"
      },
      "source": [
        "The model basically performs like it was performing with 2 correct examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUyALXHJY3ew"
      },
      "outputs": [],
      "source": [
        "# 3 incorrect examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "vUtKEtrUY4xv",
        "outputId": "b00cd9cd-2eb3-4b31-e94a-f570e23bed0e"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_lWX9YIcTaj"
      },
      "outputs": [],
      "source": [
        "#4 incorrect examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "YHaqRJf6cZoA",
        "outputId": "10b6c7b2-3da4-40bf-be84-8300f2e4c6ef"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZTF7po86WNn"
      },
      "source": [
        "Incorrect examples have no effect on the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNpNWAKwbZDr"
      },
      "source": [
        "# Fine-tuned gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86fSljuYbeef"
      },
      "outputs": [],
      "source": [
        "model_path = output_dir\n",
        "gpt2 = load_model(model_path)\n",
        "gpt2_tokenizer = load_tokenizer(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1zKMczFFIVl"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3pnIqbMFOVZ"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkyoZhP9STMy"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "vGigkDCyU7V5",
        "outputId": "0818cb6a-71cb-4200-ddc4-3bdee9b7b419"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk2bkRf7nwFA"
      },
      "source": [
        "### With incorrect examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8Bi6pUknvSR"
      },
      "outputs": [],
      "source": [
        "#one incorrect example\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "LCt19l9LnzuG",
        "outputId": "c6857f7a-9e40-4ed0-fcfc-cbb6acc23fce"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A3GIbTtoBM9"
      },
      "outputs": [],
      "source": [
        "#compare with the prediction with one correct example\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "S6toIcbgoGJW",
        "outputId": "1904a6a7-66f4-4f6b-9895-410f1dc96cb2"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJE5rdwdoOLr"
      },
      "source": [
        "When the text is preceded by an incorrect example, logit lens lets us see that the model is more uncertain about its prediction, even if it remains correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie6anZBPotRb"
      },
      "outputs": [],
      "source": [
        "# 2 incorrect examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "ur8BZSjVouiO",
        "outputId": "b9cf8b97-136e-42bc-f499-427ac3aac7fd"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LHaMMzpowOc"
      },
      "outputs": [],
      "source": [
        "# compare with 2 correct examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "3Er1IRmQo7Kx",
        "outputId": "f4a74d07-4ff6-404e-bf88-edb731228db0"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7W_fSgiplC-"
      },
      "outputs": [],
      "source": [
        "# 3 incorrect examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "uhM30KeQpqbc",
        "outputId": "1d2bdefa-b232-4ea5-c6dd-1702ed3b0d4a"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWlV2pP3psaA"
      },
      "outputs": [],
      "source": [
        "# compare it to when there are 3 correct examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "DFTCfOL1p_26",
        "outputId": "e406ab29-22a0-4ed3-e6c9-adf04ffd327b"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm5SXItEWhkO"
      },
      "outputs": [],
      "source": [
        "#4 incorrect examples\n",
        "prompt = \"\"\"Instruction: Classify the text into negative or positive sentiment.\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: A waste of time.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "W206PPzEWuA7",
        "outputId": "cde021a4-eb24-431c-efa5-66c9f63d7d3f"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFJUOG8q8hQ-"
      },
      "outputs": [],
      "source": [
        "#4 correct examples\n",
        "prompt = \"\"\"Instruction: Classify the text into negative or positive sentiment.\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "qk-2bP1c8m0O",
        "outputId": "6b4aa73a-c600-401d-bbf8-d986682fc158"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Lyo1h7qO97"
      },
      "source": [
        "When the text is preceded by incorrect examples, logit lens lets us see that the model is more uncertain about its prediction, even if it remains correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m61a4ny9Fwr"
      },
      "outputs": [],
      "source": [
        "#5 incorrect examples\n",
        "prompt = \"\"\"Instruction: Classify the text into negative or positive sentiment.\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: A waste of time.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: positive\n",
        "\n",
        "Text: Higly recommeded!\n",
        "Sentiment: negative\n",
        "\n",
        "Text: I could rewatch this film 1000 times and still enjoy it!\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "Jf940Jhs9ogo",
        "outputId": "1c3e2768-b694-43e1-eebe-b037047060e3"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, gpt2, gpt2_tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    gpt2_tokenizer,\n",
        "    gpt2_tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ehX_7H-ZBk"
      },
      "source": [
        "No matter how many incorrect example we give to the fine-tuned model, it still robustly predicts the correct label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NwfLS1dS30T"
      },
      "source": [
        "#gpt2-large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKyKvkLEU7V5"
      },
      "outputs": [],
      "source": [
        "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkjo-9NvU7V7",
        "outputId": "0d0018fe-7b39-408e-d2f2-a5b4e586734d"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX-v3wQjRrZs"
      },
      "outputs": [],
      "source": [
        "#classification\n",
        "\n",
        "#IMBD\n",
        "preds, labels = classification(test_texts[:100], model, tokenizer)\n",
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")\n",
        "\n",
        "#SST2\n",
        "preds, labels = classification(sst_texts, model, tokenizer)\n",
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")\n",
        "\n",
        "#one-shot\n",
        "\n",
        "#imdb\n",
        "preds, labels = classification([prepend_example(text,example) for text in test_texts[:100]], model, tokenizer)\n",
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7m0olXtwjZn"
      },
      "outputs": [],
      "source": [
        "#sst2\n",
        "preds, labels = classification([prepend_example(text,example) for text in sst_texts], model, tokenizer)\n",
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duU8K0_smSOo",
        "outputId": "a9adff3a-003c-4088-eea1-0a20339771ee"
      },
      "outputs": [],
      "source": [
        "#incorrect example\n",
        "#IMBD\n",
        "preds, labels = classification([prepend_example(text,incorrect_example) for text in test_texts[:100]], model, tokenizer)\n",
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")\n",
        "\n",
        "#incorrect example\n",
        "#SST2\n",
        "preds, labels = classification([prepend_example(text,incorrect_example) for text in sst_texts], model, tokenizer)\n",
        "accuracy = np.mean([1 if pred==label else 0 for pred,label in zip(preds,labels)])\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQmv4Z5ouVnP"
      },
      "outputs": [],
      "source": [
        "#4 correct examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "d5RgXkgPU7V8",
        "outputId": "01fad776-beb1-4110-d5d4-ad31543d9a8a"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, model, tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    tokenizer,\n",
        "    tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVazK8VazuIt"
      },
      "outputs": [],
      "source": [
        "#4 incorrect examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time. The plot is very boring and the actors are very bad.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "L3f0V0fezvzm",
        "outputId": "b3450421-4611-4721-9300-46a0ff20c622"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, model, tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    tokenizer,\n",
        "    tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51lTg8Kc0GPI"
      },
      "source": [
        "We are starting to see the effect of *overthinking*: the model predicts the correct example at intermediate layers but than, at the last layer, the predictions shifts to the incorrect one!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0hzXGgLzwwA"
      },
      "source": [
        "# gpt2-medium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31ObQzMOU7V8"
      },
      "outputs": [],
      "source": [
        "#gpt2-medium\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2-medium')\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2-medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSF6jf0b1mer"
      },
      "outputs": [],
      "source": [
        "#4 correct examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "PoMx-JxyU7V8",
        "outputId": "3ed4e9ec-72ab-429c-a0d5-e43b35407e34"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, model, tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    tokenizer,\n",
        "    tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0WtoEWb1qwE"
      },
      "outputs": [],
      "source": [
        "#4 incorrect examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "gTiP4dYA1vI9",
        "outputId": "4d286a8e-47e9-4a5f-e285-52704987bf4a"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, model, tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    tokenizer,\n",
        "    tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNmpXG3J2GFd"
      },
      "source": [
        "Incorrect examples make the model mistake, but there are intermediate layers where the model would have guessed correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzuC2Cqt1wBC"
      },
      "source": [
        "# Distil gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SZMJS_YU7V9"
      },
      "outputs": [],
      "source": [
        "#distil gpt2\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilgpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3Lu9NK829sL"
      },
      "outputs": [],
      "source": [
        "#4 correct examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 44
        },
        "id": "RYU44yef239F",
        "outputId": "b7fbb673-dca9-4026-e347-3017fb53fcd7"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, model, tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    tokenizer,\n",
        "    tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w59BWri3AuE"
      },
      "outputs": [],
      "source": [
        "#4 incorrect examples\n",
        "prompt = \"\"\"Classify the text into negative or positive sentiment.\n",
        "Text: A waste of time.\n",
        "Sentiment: positive\n",
        "\n",
        "Text: I saw this movie with my friends and we all loved it.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: Soooo boooring\n",
        "Sentiment: positive\n",
        "\n",
        "Text: The best movie I have ever seen. The plot is very interesting and the actors are very good.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: The movie is not bad, but it is not good either. The plot is very boring and the actors are very bad.\n",
        "Sentiment: negative\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 44
        },
        "id": "QP25yWfQU7V9",
        "outputId": "d916ee38-420c-41c8-cb23-5789944b3875"
      },
      "outputs": [],
      "source": [
        "per_layer_logits, per_layer_best_token = compute_logits(prompt, model, tokenizer)\n",
        "plot_logit_lens(\n",
        "    per_layer_logits.detach(),\n",
        "    per_layer_best_token.detach(),\n",
        "    per_layer_logits.softmax(dim=-1).detach(),\n",
        "    tokenizer,\n",
        "    tokenizer.encode(prompt, return_tensors = \"pt\"), #input_ids\n",
        "    start_ix=0,\n",
        "    layer_names=None,\n",
        "    probability=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biFdOmPHBtpO"
      },
      "source": [
        "Surprisingly, the incorrect examples make the model more confident of the right answer."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
